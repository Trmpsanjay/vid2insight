Hello everyone, my name is Jacob, and I'm an engineer at Langchain. Today, we're excited to announce the launch of Langchain 0.2.0, accompanied by a revamped documentation site for both the Python and JavaScript open-source libraries. The goal is to help you find the most relevant information more easily, whether you're a newcomer to Langchain or a seasoned user seeking guidance on advanced functionalities. Let's walk through the new structure and what it offers.

As the slides show, LangChain is a framework designed to simplify the development of applications powered by large language models, or LLMs. It streamlines every stage of the LLM application lifecycle, from initial development to deployment. Key components include LangSmith for observability, LangServe for deploying chains as REST APIs using Python, Templates for reference applications also using Python, and the core LangChain library itself, which supports both Python and JavaScript.

The sidebar is now greatly simplified, featuring only a few major sections that closely align with Diataxis principles. Let's delve into some of these sections. First, the tutorials section offers walkthroughs of popular Langchain use cases, providing starting points for beginners to grasp the framework's capabilities. Individual sections cover agents, extraction, and retrieval-augmented generation. The aim is that, by the end of each tutorial, you'll have a simple, working version of a Langchain use case. While the tutorials cover various concepts, they are not intended as deep dives into any particular one.

Specifically, the Retrieval Augmented Generation or RAG tutorial will guide you through creating a simple question-answering application using your own data. RAG enhances the knowledge of large language models by providing them with additional data, which is especially useful when you need your model to reason about information it wasn't originally trained on, like private data or information that came about after the model's training cut-off date. We'll cover the basics, including indexing your data by loading, splitting, and storing it, and then retrieving relevant information and generating answers. We'll also touch on using built-in chains within LangChain to simplify the process.

The how-to guides contain more in-depth and specific information on various LangChain features, sorted by concept. These guides are geared towards users who know what they're looking for, perhaps after building an initial version of their Langchain app. For example, if you want to add token usage tracking to more accurately track your costs, you could search for "token usage" and immediately be directed to a page answering your question.  The how-to guides also cover topics like installation, key features, and the LangChain Expression Language (LCEL), which is used to create custom chains based on the Runnable protocol.

Specifically, the guide on tracking token usage in ChatModels explains how to use the `AIMessage.response_metadata` attribute to get token usage information from your LangChain model calls. This is crucial for calculating the cost of running your application in production. The example demonstrates how to install the `langchain-openai` package, import the `ChatOpenAI` class, initialize a `ChatOpenAI` model, and then invoke the model with a simple question. Finally, you can access the token usage information through `msg.response_metadata`, which includes the number of completion tokens, prompt tokens, total tokens, the model name, system fingerprint, and the reason the model finished.

Then, there are conceptual guides, which contain broader explanations of Langchain concepts and how they fit together, including package architecture and different modules. For example, you can learn the difference between a chat model and an LLM. The conceptual guide outlines the key packages within the LangChain ecosystem, including `langchain-core`, `Partner packages`, `langchain`, and `langchain-community`, explaining their roles and relationships.

The conceptual guide also dives into the 'Runnable Interface' and the 'Chat Models' component. The Runnable Interface is designed to make it as easy as possible to create custom chains. Many LangChain components implement this protocol, including chat models, LLMs, output parsers, retrievers, and prompt templates. Chat Models use a sequence of messages as both input and output, unlike regular language models that mainly deal with plain text. While LangChain itself doesn't provide Chat Model implementations directly, it relies on third-party integrations. When constructing Chat Models, the model parameter specifies the name of the model to use.

The API references are still in the top bar. We've made an effort to lean on them more throughout the code base to reduce the amount of outdated examples around the docs. They have different sections for the different Langchain packages, as well as partner libraries too. For example, the `langchain.agents` module provides an overview of the Agent class, its purpose, and its relationship to LLMs, Chains, Tools, and Toolkits. It also outlines the class hierarchy, main helper functions, and a list of available agent classes within the module.

Finally, the integrations are in their normal spot as well, and you can get a deeper dive into different modules and different integrations for third parties that integrate with Langchain. The 'Providers' section lists various partner packages that integrate with LangChain, such as AI21, Airbyte, Amazon Web Services, Anthropic, Astra DB, Cohere, Elasticsearch, Exa Search, Fireworks, and Google.

That about wraps it up. This is not the end. We and the community want to continue to improve the content and structure of the docs as Langchain grows. For more about the refactor, you can learn more in the accompanying blog post. If you have any feedback or suggestions, we'd love for you to leave it in the GitHub discussion link in the banner here. Thank you all for listening, and happy hacking!